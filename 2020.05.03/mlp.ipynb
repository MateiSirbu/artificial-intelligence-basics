{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:33.688762Z",
     "start_time": "2020-04-15T05:32:33.683775Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:33.703722Z",
     "start_time": "2020-04-15T05:32:33.694747Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_file(path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Loads the data from the file stored at :param path: and returns the \n",
    "    input values and the class labels.\n",
    "    :param path: path of a CVS file with data\n",
    "    :return: a tuple containing the input matrix of shape (n, p) and a line \n",
    "    vector with the m class labels in {0, ..., 9}\n",
    "    \"\"\"\n",
    "    # citire date sin fisierul dat de path\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    X = df[df.columns[1:]].T\n",
    "    y = df[df.columns[0]].to_frame().T\n",
    "    assert X.ndim ==  2, 'Matrix required for input values'\n",
    "    assert y.ndim == 2, 'Column matrix required for labels'\n",
    "    assert y.shape == (1, X.shape[1]), 'Same number of lines is required'\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:33.710703Z",
     "start_time": "2020-04-15T05:32:33.706715Z"
    }
   },
   "outputs": [],
   "source": [
    "path_train = './data/mnist_train.csv'\n",
    "path_test = './data/mnist_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.182596Z",
     "start_time": "2020-04-15T05:32:33.713696Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_file(path_train)\n",
    "assert X_train.shape == (784, 60000)\n",
    "assert y_train.shape == (1, 60000)\n",
    "\n",
    "X_test, y_test = load_file(path_test)\n",
    "assert X_test.shape == (784, 10000)\n",
    "assert y_test.shape == (1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.194562Z",
     "start_time": "2020-04-15T05:32:38.184589Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_values(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Scales the values to range [0, 1].\n",
    "    :param X: an (m, n) matrix with values between 0 and 255.\n",
    "    :return: an (m, n) matrix containing values of :param X: scaled in [0, 1]\n",
    "    \"\"\"\n",
    "    result = np.array(X) / 255\n",
    "    assert 0 <= np.min(result) <= np.max(result) <= 1, 'Scaled values should be in [0, 1]'\n",
    "    assert X.shape == result.shape, 'Scaling preserves shape'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.531663Z",
     "start_time": "2020-04-15T05:32:38.203540Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = scale_values(X_train)\n",
    "assert X_train.shape == (784, 60000)\n",
    "X_test = scale_values(X_test)\n",
    "assert X_test.shape == (784, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:41:07.997557Z",
     "start_time": "2019-11-24T15:41:07.991110Z"
    }
   },
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.541636Z",
     "start_time": "2020-04-15T05:32:38.535651Z"
    }
   },
   "outputs": [],
   "source": [
    "m = 10 # number of classes\n",
    "n, p = X_train.shape\n",
    "architecture = [n, 100, m] # list: [input_size, hidden1, hidden2, ..., output_size]\n",
    "\n",
    "assert len(architecture) >= 3, 'At least one hidden layer'\n",
    "assert architecture[0] == n\n",
    "assert architecture[-1] == m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponderile sunt initializate conform strategiei lui Xavier Glorot. Pentru o matrice de ponderi $W^{[l]}$ de forma $n_{l} \\times n_{l-1}$, ponderile pot fi initializate cu o distributie uniforma in intervalul \n",
    "$$\n",
    "\\left[-\\frac{\\sqrt{6}}{\\sqrt{n_{l} + n_{l-1}}}, +\\frac{\\sqrt{6}}{\\sqrt{n_{l} + n_{l-1}}}\\right]\n",
    "$$\n",
    "\n",
    "Ponderile de bias se obisnuiesc a se initializa cu 0; intializarea aleatoare a ponderilor W este considerata suficienta pentru a obtine spargerea simetriei.\n",
    "\n",
    "Ref: [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.567564Z",
     "start_time": "2020-04-15T05:32:38.545624Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_weights(architecture: List[int], init_type:str='glorot_uniform') -> Tuple[List[np.array], List[np.array]]:\n",
    "    \"\"\"Creates the list of weights and biases for the given architecture.\n",
    "    :param architecture: list of number of nodes in each layer \n",
    "    (including input and ouotput layers)\n",
    "    :param init_type: name of initialization parameter. Defaults to \n",
    "    'glorot_uniform', add other supported initializtion strategies.\n",
    "    :return: a tuple containing: list of weight matrices W, a list of bias \n",
    "    column vectors. The two lists have the same numer of elements, number of \n",
    "    layers - 1.\n",
    "    \"\"\"\n",
    "    L = len(architecture)\n",
    "    W, b = [], []\n",
    "    # initializare de ponderi\n",
    "    for n_lplus1, nl in zip(architecture[1:], architecture[:-1]):\n",
    "        W.append(np.random.uniform(- np.sqrt(6)/np.sqrt(nl + n_lplus1), np.sqrt(6)/np.sqrt(nl + n_lplus1), (n_lplus1, nl)))\n",
    "    for n_l in architecture[1:]:\n",
    "        b.append(np.vstack(np.repeat(0, n_l)))\n",
    "    assert len(W) == len(b) == L-1\n",
    "    for i, w in enumerate(W):\n",
    "        assert w.shape == (architecture[i+1], architecture[i]), f'Shape of W[{i}] should be (L[{i+1}], L[{i}])'\n",
    "    for i, _b in enumerate(b):\n",
    "        assert _b.shape == (architecture[i+1], 1), f'Shape of b[{i}] should be (L[{i+1}], 1)'\n",
    "    if init_type == 'glorot_uniform':\n",
    "        for i, w in enumerate(W):\n",
    "            w_shape_sum = np.sum(w.shape)\n",
    "            assert -np.sqrt(6)/np.sqrt(w_shape_sum) <= np.min(w) <= np.sqrt(6)/np.sqrt(w_shape_sum), f\"Values of W[{i}] should be according to Glorot's initialization\"\n",
    "        for i, _b in enumerate(b):\n",
    "            assert 0 == np.min(_b) == np.min(_b) == 0, f\"Values of b[{i}] should be initialized to 0\"\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions:\n",
    "- Logistic sigmoid:\n",
    "$$f = \\sigma : \\mathbb{R} \\rightarrow (0, 1), f(z) = \\sigma(z) = \\frac{1}{1 + \\exp{(-z)}}$$\n",
    "and its derivative:\n",
    "$$f'(z) = \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) = f(z) \\cdot (1 - f(z))$$\n",
    "- Hyperbolic tangent:\n",
    "$$f = \\tanh : \\mathbb{R} \\rightarrow (-1, 1), f(z) = \\tanh(z) = \\frac{\\exp(z) - \\exp(-z)}{\\exp(z) + \\exp(-z)}$$\n",
    "and its derivative:\n",
    "$$f'(z) = \\tanh'(z) = 1 - \\tanh^2(z) = 1 - f^2(z)$$\n",
    "- Rectified Linear Unit (ReLU):\n",
    "$$f(x) = \\max(0, x) = \\begin{cases} 0 & \\text{if } x \\leq 0 \\\\ x & \\text{if } x > 0 \\end{cases}$$\n",
    "and its derivative:\n",
    "$$f'(x) = \\begin{cases} 0 & \\text{if } x \\leq 0 \\\\ 1 & \\text{if } x > 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.584519Z",
     "start_time": "2020-04-15T05:32:38.569559Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z: np.array) -> np.array:\n",
    "    \"\"\"Computes sigmoid activation function\"\"\"\n",
    "    return 1/1+np.exp(-z)\n",
    "\n",
    "def derivative_sigmoid(z: np.array) -> np.array:\n",
    "    \"\"\"Computes the derivatives for the sigmoid activation function\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def tanh(z: np.array) -> np.array:\n",
    "    \"\"\"Computes the tanh activation function\"\"\"\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "def derivative_tanh(z: np.array) -> np.array:\n",
    "    \"\"\"Computes the derivatives for the tanh activation function\"\"\"\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "def ReLU(z: np.array) -> np.array:\n",
    "    \"\"\"Computes the rectified linear unit activation function\"\"\"\n",
    "    return (z > 0) * z \n",
    "\n",
    "def derivative_ReLU(z: np.array) -> np.array:\n",
    "    \"\"\"Computes the derivatives of the rectified linear unit activation function\"\"\"\n",
    "    return np.heaviside(z, 0) # one-liner of thr = 0, z[z <= 0] = 0, z[z == 0] = thr, z[z > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.596496Z",
     "start_time": "2020-04-15T05:32:38.589506Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z, axis=0):\n",
    "    \"\"\"Applies softmax to a matrix z.\n",
    "    :param z: np.array of shape (m, k)\n",
    "    \"\"\"\n",
    "    max_z = np.max(z, axis=axis, keepdims=True)\n",
    "    exp_z = np.exp(z-max_z)\n",
    "    sum_exp_z = np.sum(np.exp(z-max_z), axis=axis, keepdims=True)\n",
    "    result = exp_z/sum_exp_z\n",
    "    assert np.allclose(np.sum(result, axis=axis), 1.0)\n",
    "    return result\n",
    "def derivative_softmax(z):\n",
    "    \"\"\"Computes the deriative of the softmax function.\"\"\"\n",
    "    return softmax(z)*(1 - softmax(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.609453Z",
     "start_time": "2020-04-15T05:32:38.599481Z"
    }
   },
   "outputs": [],
   "source": [
    "W, b = create_weights(architecture=architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:54:14.739424Z",
     "start_time": "2020-04-15T15:54:14.320545Z"
    }
   },
   "outputs": [],
   "source": [
    "def can_multiply(a:np.array, b:np.array) -> bool:\n",
    "    return a.ndim == b.ndim == 2 and a.shape[1] == b.shape[0]\n",
    "\n",
    "def can_multiply_hadamard(a:np.array, b:np.array) -> bool:\n",
    "    return a.shape == b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulele 6.18 - 6.21 din curs:\n",
    "$$\\Large \\textbf{a}^{(1)} = \\textbf{x}$$\n",
    "$$\\Large \\textbf{z}^{(l)} = \\textbf{W}^{(l-1)} \\cdot \\textbf{a}^{(l-1)} + \\textbf{b}^{(l-1)}$$\n",
    "$$\\Large \\textbf{a}^{(l)} = f^{(l)}(\\textbf{z}^{(l)}), f^{(l)}(\\cdot), l = 2, ..., L$$\n",
    "$$\\Large \\textbf{o} = \\textbf{a}^{(L)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.636381Z",
     "start_time": "2020-04-15T05:32:38.624413Z"
    }
   },
   "outputs": [],
   "source": [
    "def model(X:np.array, W:List[np.array], b:List[np.array], f:List[Callable]) -> (np.array, np.array):\n",
    "    \"\"\"Computes the output produced by the MLP for the given input X\n",
    "    :param X: np.array of shape (n, p). Each column of X is a datum from a set.\n",
    "    :param W: a list of weight matrices\n",
    "    :param b: a list of bias columns\n",
    "    :param f: a list of activation functions\n",
    "    :return: a matrix of output values produced by MLP, of shape: number of \n",
    "    predicted outputs (e.g. classes), number of input vectors p\n",
    "    \"\"\"\n",
    "    Z = []\n",
    "    A = []\n",
    "    assert len(W) == len(b) == len(f)\n",
    "    p = X.shape[1]\n",
    "    a = X\n",
    "    A.append(a)\n",
    "    for i, (_w, _b, _f) in enumerate(zip(W, b, f)):\n",
    "        # variabila i poate fi folosita pentru debug\n",
    "        assert can_multiply(_w, a)\n",
    "        z = _w @ a + _b\n",
    "        Z.append(z)\n",
    "        assert z.shape == (_w.shape[0], p)\n",
    "        a = _f(z)\n",
    "        A.append(a)\n",
    "        assert a.shape == z.shape\n",
    "    assert a.shape == (W[-1].shape[0], p)\n",
    "    return a, Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:38.852801Z",
     "start_time": "2020-04-15T05:32:38.639373Z"
    }
   },
   "outputs": [],
   "source": [
    "# f[0] = functia de activare pe primul strat ascuns; \n",
    "# f[1] = functia de activare pe al doilea strat ascuns etc.\n",
    "f = [sigmoid, softmax]\n",
    "y_hat, _, _ = model(X_train, W, b, f)\n",
    "\n",
    "assert y_hat.shape == (m, p)\n",
    "assert np.allclose(y_hat.sum(axis=0), np.ones(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large J(\\textbf{W}, \\textbf{b}) = - \\sum_{i=1}^p \\sum_{j=1}^m d_j^{(i)} \\log o_j^{(i)} + \\frac{\\lambda}{2} \\sum_{l=1}^{L-1} \\left\\lVert \\textbf{W}^{(l)} \\right\\rVert_F^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T05:32:44.535803Z",
     "start_time": "2020-04-15T05:32:44.527825Z"
    }
   },
   "outputs": [],
   "source": [
    "def J(X, y, W, b, f, num_classes=10, _lambda=0.01):\n",
    "    \"\"\"Computes the error function for MLP\n",
    "    :param X: np.array of shape (n, k)\n",
    "    :param y: np.array of shape (1, k)\n",
    "    :param W: list pf MLPs weights\n",
    "    :param b: list pf MLPs biases\n",
    "    :return: loss values, composed of cross entropy + penalty term\n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    EPS = 1e-5\n",
    "    # computes a one hot encoding for the given classes:\n",
    "    # if y[i]=c, 0 <= c <= 9 (here), then column i in one_hot_encoding is filled\n",
    "    # in with 0, excepting line c where one finds value 1\n",
    "    y_ohe = np.zeros((num_classes, p))\n",
    "    y_ohe[y, np.arange(y_ohe.shape[1])] = 1\n",
    "    assert np.all(y_ohe.sum(axis=0) == 1)\n",
    "    predicted, _, _ = model(X, W, b, f)\n",
    "    predicted = np.clip(predicted, EPS, 1-EPS)\n",
    "    loss1 = -np.sum(y_ohe * np.log(predicted))\n",
    "    W_frobenius_norms = [np.linalg.norm(w, ord='fro') for w in W]\n",
    "    loss2 = _lambda/2 * np.sum(W_frobenius_norms)\n",
    "    return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T05:08:15.074986Z",
     "start_time": "2020-04-02T05:08:15.069005Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(X:np.array, y:np.array, W: List[np.array], b: List[np.array], f:List[Callable]) -> float:\n",
    "    \"\"\"Computes the accuracy on a given input dataset X, with ground truth y\n",
    "    :param X: np.array of shape (n, k)\n",
    "    :param y: np.array of shape (1, k); each value is the index of a class\n",
    "    :param W: list of MLP's weights\n",
    "    :param b: list of MLP's biases\n",
    "    :param f: list of activation functions. the last one must be softmax\n",
    "    :return: ratio between correctly classified vectors and total number of cases\n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    y_hat, _, _ = model(X, W, b, f)\n",
    "    y_ohe = np.zeros((10, p))\n",
    "    y_ohe[y, np.arange(y_ohe.shape[1])] = 1\n",
    "    y_predicted = np.zeros_like(y_hat)\n",
    "    y_predicted[y_hat.argmax(axis=0), np.arange(y_hat.shape[1])] = 1\n",
    "    return (np.logical_and(y_predicted == y_ohe, y_ohe == 1)).sum() / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(architecture: List[int], X: np.array, d: np.array, W: List[np.array], b:List[np.array], num_classes, _lambda:float, alpha:float):\n",
    "    DW, Db = [], []\n",
    "    fderiv = [derivative_sigmoid, derivative_softmax]\n",
    "    for n_lplus1, nl in zip(architecture[1:], architecture[:-1]):\n",
    "        DW.append(np.zeros((n_lplus1, nl)))\n",
    "    for n_l in architecture[1:]:\n",
    "        Db.append(np.vstack(np.repeat(0, n_l)))\n",
    "    p = X.shape[1]\n",
    "    L = len(architecture)\n",
    "    \n",
    "    for digit in range(0, p):\n",
    "        d_ohe = np.zeros((num_classes, 1))\n",
    "        d_ohe[d[digit], 0] = 1\n",
    "        o_ohe, Z, A = model(X[:,digit].reshape(-1,1), W, b, f)\n",
    "        delta = []\n",
    "        \n",
    "        delta.append(-(d_ohe - o_ohe) * fderiv[L-2](Z[L-2]))\n",
    "        \n",
    "        for l in range(L-2, 0, -1):\n",
    "            delta.append((W[l].T @ delta[L-2-l]) * fderiv[l](Z[l-1]))\n",
    "        \n",
    "        dJdW, dJdb = [], []\n",
    "        for l in range(0, L-2):\n",
    "            dJdW.append(delta[L-2-l] @ A[l].T)\n",
    "            dJdb.append(delta[L-2-l])\n",
    "\n",
    "        for l in range(0, L-2):\n",
    "            DW[l] = DW[l] + dJdW[l]\n",
    "            Db[l] = Db[l] + dJdb[l]\n",
    "\n",
    "        for l in range(0, L-2):\n",
    "            W[l] = W[l] - alpha * ((1/p * DW[l]) + _lambda * W[l])\n",
    "            b[l] = b[l] - alpha * (1/p * Db[l])\n",
    "        \n",
    "        if digit % 1000 == 0:\n",
    "            print(f\"DIGIT: {digit}\")\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T05:08:15.098922Z",
     "start_time": "2020-04-02T05:08:15.078976Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(architecture: List[int], X_train: np.array, y_train: np.array, X_test: np.array, y_test: np.array, num_classes, W: List[np.array], b:List[np.array], f:List[Callable], _lambda: float, alpha: float, max_delta_error:float=1e-4) -> Tuple[List[np.array], List[np.array], List[float], List[float], List[float]]:\n",
    "    \"\"\"Runs the training on the training dataset (X, y). Stops when  \n",
    "    difference between  two succesive error values is lower than :param max_delta_error:\n",
    "    :param X_train: np.array of shape (n, k), with training cases. Each column is a training case\n",
    "    :param y_train: np.array of shape (1, k), containing labels (0=class 0, ...)\n",
    "    :param X_test: np.array of shape (n, l), with test cases. Each column is a test vector\n",
    "    :param y_test: np.array of shape (1, l), containing labels (0=class 0, ...)\n",
    "    :param num_classes: number of classes\n",
    "    :param W: list of MLP's weights\n",
    "    :param b: list of MLP's biases\n",
    "    :param f: list of activations functions; the last one must be softmax\n",
    "    :param _lambda: coefficient >= for the L2 penalty term\n",
    "    :param alpha: > 0, learning rate\n",
    "    :max_delta_error: >0, a threshold for max absolute difference of succesive loss values\n",
    "    :return: a tuple consisting of: list of weight matrices, list of biases, list of errors computed at each epoch on training set, 2 lists of accuracies on training and on test set at each epoch\n",
    "    \"\"\"\n",
    "    errors = [J(X_train, y_train, W, b, f, num_classes, _lambda)]\n",
    "    acc_train = [accuracy(X_train, y_train, W, b, f)]\n",
    "    acc_test = [accuracy(X_test, y_test, W, b, f)]\n",
    "    epoch = 0\n",
    "    p = X_train.shape[1]\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        W, b = backpropagation(architecture, X_train, y_train, W, b, num_classes, _lambda, alpha)\n",
    "        # actualizare ponderi si biases W, b pentru fiecare pereche de date din setul de instruire *_test\n",
    "        error = J(X_train, y_train, W, b, f, num_classes, _lambda)\n",
    "        errors.append(error)\n",
    "        train_acc = accuracy(X_train, y_train, W, b, f)\n",
    "        acc_train.append(train_acc)\n",
    "        test_acc = accuracy(X_test, y_test, W, b, f)\n",
    "        acc_test.append(test_acc)\n",
    "        ##if epoch % 10 == 0: TO UNCOMMENT!!!!!\n",
    "        print(f'Epoch: {epoch}, error: {error}, train accuracy: {train_acc}, test accuracy: {test_acc}')\n",
    "        if np.abs(errors[-1] - errors[-2]) < max_delta_error:\n",
    "            break\n",
    "        # plot de valore de eroare pe train si pe test\n",
    "    return W, b, errors, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T13:44:41.944041Z",
     "start_time": "2020-04-02T05:52:03.345797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIGIT: 0\n",
      "DIGIT: 1000\n",
      "DIGIT: 2000\n",
      "DIGIT: 3000\n",
      "DIGIT: 4000\n",
      "DIGIT: 5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-8954bbb98e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-72a5d4ba5ac7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(architecture, X_train, y_train, X_test, y_test, num_classes, W, b, f, _lambda, alpha, max_delta_error)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# actualizare ponderi si biases W, b pentru fiecare pereche de date din setul de instruire *_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-5e43201a4a6d>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(architecture, X, d, W, b, num_classes, _lambda, alpha)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfderiv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#dJdW, dJdb = [], []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-a49539182469>\u001b[0m in \u001b[0;36mderivative_softmax\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mderivative_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"\"\"Computes the deriative of the softmax function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-a49539182469>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(z, axis)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msum_exp_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_z\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum_exp_z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mderivative_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/unitbv/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m     \"\"\"\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mequal_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misclose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/unitbv/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0mfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxfin\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0myfin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/unitbv/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mwithin_tol\u001b[0;34m(x, y, atol, rtol)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m     \"\"\"\n\u001b[0;32m-> 2244\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2245\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W, b = create_weights(architecture)\n",
    "\n",
    "W, b, errors, acc_train, acc_test = train(architecture, X_train, y_train, X_test, y_test, 10, W, b, f, 0.01, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:03:19.902932Z",
     "start_time": "2020-04-02T14:03:19.673520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(errors, label='Loss on train DS')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:08:45.594232Z",
     "start_time": "2020-04-02T14:08:45.419907Z"
    }
   },
   "outputs": [],
   "source": [
    "# acc_train, acc_test\n",
    "plt.plot(acc_train, label='Acc train')\n",
    "plt.plot(acc_test, label='Acc test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T05:12:22.914348Z",
     "start_time": "2020-04-02T05:12:22.885426Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy on test set: {accuracy(X_test, y_test, theta)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
